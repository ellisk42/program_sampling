\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{graves2014neural}
\citation{lake2015human,DBLP:conf/icml/LiangJK10,menon2013machine}
\citation{solar2008program}
\citation{ermon2013embed}
\citation{solar2008program}
\newlabel{introduction}{{1}{1}{}{section.1}{}}
\newlabel{ambiguous}{{1}{1}{Learning string manipulation programs by example (top input/output pair). Our system receives data like that shown above and then sampled the different programs shown below (among others) from a description-length prior}{figure.1}{}}
\citation{gomes2006near,ermon2013embed,ermon2012uniform,chakraborty2014balancing}
\citation{solar2008program,Gulwani:2011:SLP:1993498.1993506}
\citation{ermon2013embed,gomes2006near,chakraborty2014balancing}
\citation{chakraborty2014distribution}
\citation{chakraborty2014distribution}
\citation{gomes2006near,valiant1985np,chakraborty2014balancing,gomes2006model}
\newlabel{cartoon}{{2}{2}{\theSystem {} twice distorts the distribution $p(\cdot )$. First it introduces a parameter $d$ that bounds the tilt; we correct for this by accepting samples w.p. $A(x)$. Second it samples from $q(\cdot )$ by drawing instead from $r(\cdot )$, where $KL(q||r)$ can be made arbitrarily small by appropriately setting another parameter, $K$. $A(x)r(x)$ is distribution of samples}{figure.2}{}}
\citation{chakraborty2014distribution}
\citation{ermon2013embed}
\citation{gomes2006near}
\citation{gomes2006near,valiant1985np,chakraborty2014balancing,gomes2006model}
\citation{gomes2006near}
\newlabel{acceptanceBound}{{1}{3}{}{proposition.1}{}}
\newlabel{propositionLowerBound}{{2}{3}{}{proposition.2}{}}
\citation{ermon2013embed}
\citation{chakraborty2014distribution,chakraborty2014balancing}
\citation{ermon2013embed}
\citation{singh2013automated}
\citation{gomes2006model}
\newlabel{mainResult}{{3}{4}{}{proposition.3}{}}
\newlabel{lowerBound}{{18}{4}{}{equation.2.18}{}}
\newlabel{ranktheorem}{{4}{4}{}{proposition.4}{}}
\citation{solar2006combinatorial}
\citation{crypto}
\citation{real_world}
\citation{Gulwani:2011:ASP:1926385.1926423}
\newlabel{mainAlgorithm}{{1}{5}{}{algorithm.1}{}}
\newlabel{eqmatrix}{{24}{5}{}{equation.2.24}{}}
\newlabel{coordinateTransform}{{25}{5}{}{equation.2.25}{}}
\newlabel{proposition:tt}{{5}{5}{}{proposition.5}{}}
\citation{DBLP:conf/ecai/LinDETM14}
\citation{ermon2013embed}
\citation{solar2008program}
\citation{gulwani2011automating}
\citation{raychev2016learning,ellis2015unsupervised,singh2013automated}
\newlabel{heat}{{3}{6}{Accuracy (colored contours) vs Performance (monochrome cells) trade-off for a program synthesis problem; upper bounds plotted. Performance measured in expected solver invocations; accuracy measured in log KL divergence. Prop.~\ref {acceptanceBound} lower bounds the tilt of performant samplers, while Prop.~\ref {propositionLowerBound} upper bounds $K$ to $O(d)$, forcing our sampler into the darker (faster) regions. KL divergence falls off exponentially fast in $\Delta = O(d - K)$, (Prop.~\ref {mainResult}) while solver invocations grows exponentially in $\Delta $ (Prop.~\ref {proposition:tt}) but is bounded by $|X|$ (Prop.~\ref {ranktheorem}), shown in white}{figure.3}{}}
\newlabel{flashPerformance}{{4}{6}{Generalization when learning text edit operations by example. Results averaged across 19 problems. Solid: 100 samples from \theSystem {} . Dashed: enumerating 100 programs. Dotted: MDL learner. Test cases past 1 (resp. 2,3) examples are held out when trained on 1 (resp. 2,3) examples}{figure.4}{}}
\newlabel{mdl}{{5}{7}{Comparing the MDL learner (dashed black line) to program sampling when doing one-shot learning. We count a problem as ``solved'' if the correct joint prediction to the test cases is in the top $C$ most frequent samples}{figure.5}{}}
\newlabel{listTimes}{{1}{7}{Average solver time to generate a sample measured in seconds. See Fig.~\ref {listCurves} and \ref {flashPerformance} for training set sizes}{table.1}{}}
\newlabel{listCurves}{{6}{7}{Generalization performance of the program learner on list manipulation tasks. Trained on random lists of length $\leq 3$}{figure.6}{}}
\newlabel{marginal}{{7}{7}{Sampling frequency vs. ground truth probability for 1000 runs of \theSystem {} on a counting (see Fig.~\ref {listCurves}) task}{figure.7}{}}
\citation{DBLP:books/daglib/0070933}
\citation{schkufza2013stochastic}
\citation{DBLP:conf/icml/LiangJK10,menon2013machine,Dechter:2013:BLV:2540128.2540316}
\citation{DBLP:journals/corr/ReedF15,graves2014neural}
\citation{raychev2016learning,ellis2015unsupervised,singh2013automated}
\citation{gomes2006near,gomes2006model}
\citation{valiant1985np}
\citation{solar2008program}
\citation{nori2015efficient,schkufza2013stochastic,DBLP:books/daglib/0070933}
\citation{ellis2015unsupervised,raychev2016learning}
\citation{gomes2006model}
\citation{liang11dcs}
\citation{lake2015human}
\citation{logical}
\bibdata{main}
\bibcite{crypto}{{1}{}{{cry}}{{}}}
\bibcite{chakraborty2014distribution}{{2}{2014{a}}{{Chakraborty et~al.}}{{Chakraborty, Fremont, Meel, Seshia, and Vardi}}}
\bibcite{chakraborty2014balancing}{{3}{2014{b}}{{Chakraborty et~al.}}{{Chakraborty, Meel, and Vardi}}}
\bibcite{Dechter:2013:BLV:2540128.2540316}{{4}{2013}{{Dechter et~al.}}{{Dechter, Malmaud, Adams, and Tenenbaum}}}
\bibcite{ellis2015unsupervised}{{5}{2015}{{Ellis et~al.}}{{Ellis, Solar-Lezama, and Tenenbaum}}}
\bibcite{ermon2012uniform}{{6}{2012}{{Ermon et~al.}}{{Ermon, Gomes, and Selman}}}
\bibcite{ermon2013embed}{{7}{2013}{{Ermon et~al.}}{{Ermon, Gomes, Sabharwal, and Selman}}}
\bibcite{gomes2006model}{{8}{2006{a}}{{Gomes et~al.}}{{Gomes, Sabharwal, and Selman}}}
\bibcite{gomes2006near}{{9}{2006{b}}{{Gomes et~al.}}{{Gomes, Sabharwal, and Selman}}}
\bibcite{graves2014neural}{{10}{2014}{{Graves et~al.}}{{Graves, Wayne, and Danihelka}}}
\bibcite{Gulwani:2011:ASP:1926385.1926423}{{11}{2011{a}}{{Gulwani}}{{}}}
\bibcite{gulwani2011automating}{{12}{2011{b}}{{Gulwani}}{{}}}
\bibcite{Gulwani:2011:SLP:1993498.1993506}{{13}{2011}{{Gulwani et~al.}}{{Gulwani, Jha, Tiwari, and Venkatesan}}}
\bibcite{real_world}{{14}{2015}{{Gulwani et~al.}}{{Gulwani, Hernandez-Orallo, Kitzelmann, Muggleton, Schmid, and Zorn}}}
\bibcite{logical}{{15}{2008}{{Katz et~al.}}{{Katz, Goodman, Kersting, Kemp, and Tenenbaum}}}
\bibcite{DBLP:books/daglib/0070933}{{16}{1993}{{Koza}}{{}}}
\bibcite{lake2015human}{{17}{2015}{{Lake et~al.}}{{Lake, Salakhutdinov, and Tenenbaum}}}
\bibcite{liang11dcs}{{18}{2011}{{Liang et~al.}}{{Liang, Jordan, and Klein}}}
\bibcite{DBLP:conf/icml/LiangJK10}{{19}{2010}{{Liang et~al.}}{{Liang, Jordan, and Klein}}}
\bibcite{DBLP:conf/ecai/LinDETM14}{{20}{2014}{{Lin et~al.}}{{Lin, Dechter, Ellis, Tenenbaum, and Muggleton}}}
\bibcite{menon2013machine}{{21}{2013}{{Menon et~al.}}{{Menon, Tamuz, Gulwani, Lampson, and Kalai}}}
\bibcite{nori2015efficient}{{22}{2015}{{Nori et~al.}}{{Nori, Ozair, Rajamani, and Vijaykeerthy}}}
\bibcite{raychev2016learning}{{23}{2016}{{Raychev et~al.}}{{Raychev, Bielik, Vechev, and Krause}}}
\bibcite{DBLP:journals/corr/ReedF15}{{24}{2015}{{Reed \& de~Freitas}}{{Reed and de~Freitas}}}
\bibcite{schkufza2013stochastic}{{25}{2013}{{Schkufza et~al.}}{{Schkufza, Sharma, and Aiken}}}
\bibcite{singh2013automated}{{26}{2013}{{Singh et~al.}}{{Singh, Gulwani, and Solar-Lezama}}}
\bibcite{solar2008program}{{27}{2008}{{Solar~Lezama}}{{}}}
\bibcite{solar2006combinatorial}{{28}{2006}{{Solar-Lezama et~al.}}{{Solar-Lezama, Tancau, Bodik, Seshia, and Saraswat}}}
\bibcite{valiant1985np}{{29}{1985}{{Valiant \& Vazirani}}{{Valiant and Vazirani}}}
\bibstyle{icml2016}
